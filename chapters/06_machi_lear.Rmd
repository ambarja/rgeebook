# (PART) Machine Learning {-}

```{r, include = FALSE}
source("common.R")
```


# Overview of ML in Earth Engine {-}

## Machine Learning in Earth Engine {-}

Machine Learning (ML) in Earth Engine is supported with:

  - EE API methods in the __ee$Classifier__, __ee$Clusterer__, or __ee$Reducer__ packages for training and inference within Earth Engine.
  - Export and import functions for TFRecord files to facilitate TensorFlow model development. Inference using data in Earth Engine and a trained model hosted on Google's AI Platform is supported with the __ee$Model__ package.

### EE API methods  {-}

Training and inference using ee.Classifier or ee.Clusterer is generally effective up to a request size of approximately 100 megabytes. As a very rough guideline, assuming 32-bit (i.e. float) precision, this can accommodate training datasets that satisfy (where n is number of examples and b is the number of bands):
$$ nb ≤ (100 * 2^{20}) / 4 $$

This is only an approximate guideline due to additional overhead around the request, but note that for $b = 100$ (i.e. you have 100 properties used for prediction), $n ≅ 200,000$. Since Earth Engine processes 256x256 image tiles, inference requests on imagery must have $b < 400$ (again assuming 32-bit precision of the imagery). Examples of machine learning using the Earth Engine API can be found on the [Supervised Classification]() page or the [Unsupervised Classification]() page. Regression is generally performed with an __ee$Reducer__ as described on [this page](), but see also __ee$Reducer$RidgeRegression__.

### TensorFlow
If you require more complex models, larger training datasets, more input properties or longer training times, then [TensorFlow]() is a better option. TensorFlow models are developed, trained and deployed outside Earth Engine. For easier interoperability, the Earth Engine API provides methods to import/export data in [TFRecord]() format. This facilitates generating training/evaluation data in Earth Engine and exporting them to a format where they can be readily consumed by a TensorFlow model. To perform prediction with a trained TensorFlow model, you can either export imagery in TFRecord format then import the predictions (also in TFRecord) to Earth Engine, or you can [deploy your trained model to Google AI Platform]() and perform inference directly in Earth Engine using __ee$Model$fromAiPlatformPredictor__.

See [the TensorFlow page]() for details and example workflows.

# Supervised Classification Algorithms

The **Classifier** package handles supervised classification by traditional ML algorithms running in Earth Engine. These classifiers include CART, RandomForest, NaiveBayes and SVM. The general workflow for classification is:

  1. Collect training data. Assemble features which have a property that stores the known class label and properties storing numeric values for the predictors.
  
  2. Instantiate a classifier. Set its parameters if necessary.
  
  3. Train the classifier using the training data.
  
  4. Classify an image or feature collection.
  
  5. Estimate classification error with independent validation data.
  
```{r setup, include=FALSE}
library(vembedr)
knitr::opts_chunk$set(echo = TRUE)
```

<center>
```{r, echo=FALSE}
embed_youtube("NPplRtH2N94")
```
</center>

The training data is a **FeatureCollection** with a property storing the class label and properties storing predictor variables. Class labels should be consecutive, integers starting from 0. If necessary, use **remap()** to convert class values to consecutive integers. The predictors should be numeric.

Training and/or validation data can come from a variety of sources. To collect training data interactively in Earth Engine, you can use the geometry drawing tools (see the [geometry tools section of the Code Editor page]()). Alternatively, you can import predefined training data from an Earth Engine table asset (see the [Importing Table Data page]() for details). Get a classifier from one of the constructors in __ee$Classifier__. Train the classifier using __classifier$train()__. Classify an **Image** or __FeatureCollection__ using **classify()**. The following example uses a Classification and Regression Trees (CART) classifier ([Breiman et al. 1984]()) to predict three simple classes:

```{r, eval=FALSE}
library(rgee)

ee_Initialize()

# Make a cloud-free Landsat 8 TOA composite (from raw imagery).
l8 <- ee$ImageCollection("LANDSAT/LC08/C01/T1")

image <- ee$Algorithms$Landsat$simpleComposite(
  collection = l8$filterDate("2018-01-01", "2018-12-31"),
  asFloat = TRUE
)

# Use these bands for prediction.
bands <- c("B2", "B3", "B4", "B5", "B6", "B7", "B10", "B11")

# Load training points. The numeric property 'class' stores known labels.
points <- ee$FeatureCollection("GOOGLE/EE/DEMOS/demo_landcover_labels")

# This property stores the land cover labels as consecutive
# integers starting from zero.
label <- "landcover"


#Overlay the points on the imagery to get training.
training <- image$select(bands)$sampleRegions(
  collection = points, 
  properties = list(label), 
  scale = 30
)

# Train a CART classifier with default parameters.
trained <- ee$Classifier$smileCart()$train(training, label, bands)

# Classify the image with the same bands used for training.
classified <- image$select(bands)$classify(trained)

# Display the inputs and the results.
Map <- R6Map$new()
Map$centerObject(points$geometry(), 11)
Map$addLayer(
  eeObject = image,
  visParams = list(bands = c("B4", "B3", "B2"), max = 0.4),
  name = "image",
  position = "right"
)
Map$addLayer(
  eeObject = classified,
  visParams = list(min = 0, max = 2, palette = c("red", "green", "blue")),
  name = "classification",
  position = "left"
)
Map
```

<center>
<img src="./images/chapter_06/figure_01.png" width=95%>
</center>

In this example, the training points in the table store only the class label. Note that the training property (**'landcover'**) stores consecutive integers starting at 0 (Use [remap()]() on your table to turn your class labels into consecutive integers starting at zero if necessary). Also note the use of __image$sampleRegions()__ to get the predictors into the table and create a training dataset. To train the classifier, specify the name of the class label property and a list of properties in the training table which the classifier should use for predictors. The number and order of the bands in the image to be classified must exactly match the order of the properties list provided to __classifier$train()__. Use __image$select()__ to ensure that the classifier schema matches the image.

If the training data are polygons representing homogeneous regions, every pixel in each polygon is a training point. You can use polygons to train as illustrated in the following example:

```{r, eval=FALSE}
library(rgee)
ee_Initialize('junior')
# Make a cloud-free Landsat 8 TOA composite (from raw imagery).
l8 <-  ee$ImageCollection('LANDSAT/LC08/C01/T1')

image <- ee$Algorithms$Landsat$simpleComposite(
  collection = l8$filterDate('2018-01-01', '2018-12-31'), 
  asFloat=TRUE
)

# Use these bands for prediction.
bands <- c('B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11')

# Manually created polygons.
forest1 <- ee$Geometry$Rectangle(-63.0187, -9.3958, -62.9793, -9.3443)
forest2 <- ee$Geometry$Rectangle(-62.8145, -9.206, -62.7688, -9.1735)
nonForest1 <- ee$Geometry$Rectangle(-62.8161, -9.5001, -62.7921, -9.4486)
nonForest2 <- ee$Geometry$Rectangle(-62.6788, -9.044, -62.6459, -8.9986)


# Make a FeatureCollection from the hand-made geometries.
polygons = ee$FeatureCollection(c(
  ee$Feature(nonForest1, list(class = 0)),
  ee$Feature(nonForest2, list(class = 0)),
  ee$Feature(forest1, list(class = 1)),
  ee$Feature(forest2, list(class = 1))
))

# Get the values for all pixels in each polygon in the training.
training <- image$sampleRegions(
  # Get the sample from the polygons FeatureCollection.
  collection = polygons,
  # Keep this list of properties from the polygons.
  properties = list('class'),
  # Set the scale to get Landsat pixels in the polygons.
  scale = 30
)

# Create an SVM classifier with custom parameters.
classifier <- ee$Classifier$libsvm(
  kernelType= 'RBF',
  gamma = 0.5,
  cost = 10
)

# Train the classifier.
trained <- classifier$train(training, 'class', bands)

# Classify the image.
classified <-  image$classify(trained)

# Display the classification result and the input image.
Map$setCenter(-62.836, -9.2399, 9)
Map$addLayer(image, list(bands = c('B4', 'B3', 'B2'), max = 0.5, gamma = 2))
Map$addLayer(polygons, {}, 'training polygons')
Map$addLayer(classified,
             list(min = 0, max = 1, palette = c('red', 'green')),
             'deforestation');
```

This example uses a Support Vector Machine (SVM) classifier ([Burges 1998](http://rd.springer.com/article/10.1023%2FA%3A1009715923555)). Note that the SVM is specified with a set of custom parameters. Without a priori information about the physical nature of the prediction problem, optimal parameters are unknown. See [Hsu et al. (2003)](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf) for a rough guide to choosing parameters for an SVM.

## Accuracy Assessment

To assess the accuracy of a classifier, use a **ConfusionMatrix** ([Stehman 1997](http://www.sciencedirect.com/science/article/pii/S0034425797000837)). The following example uses **sample()** to generate training and validation data from a MODIS reference image and compares confusion matrices representing training and validation accuracy:

```{r, eval=FALSE}
library(rgee)
ee_Initialize('junior')
# Make a cloud-free Landsat 8 TOA composite (from raw imagery).
l8 <-  ee$ImageCollection('LANDSAT/LC08/C01/T1')

image <- ee$Algorithms$Landsat$simpleComposite(
  collection = l8$filterDate('2018-01-01', '2018-12-31'), 
  asFloat=TRUE
)

# Use these bands for prediction.
bands <- c('B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B10', 'B11')

# Manually created polygons.
forest1 <- ee$Geometry$Rectangle(-63.0187, -9.3958, -62.9793, -9.3443)
forest2 <- ee$Geometry$Rectangle(-62.8145, -9.206, -62.7688, -9.1735)
nonForest1 <- ee$Geometry$Rectangle(-62.8161, -9.5001, -62.7921, -9.4486)
nonForest2 <- ee$Geometry$Rectangle(-62.6788, -9.044, -62.6459, -8.9986)


# Make a FeatureCollection from the hand-made geometries.
polygons = ee$FeatureCollection(c(
  ee$Feature(nonForest1, list(class = 0)),
  ee$Feature(nonForest2, list(class = 0)),
  ee$Feature(forest1, list(class = 1)),
  ee$Feature(forest2, list(class = 1))
))

# Get the values for all pixels in each polygon in the training.
training <- image$sampleRegions(
  # Get the sample from the polygons FeatureCollection.
  collection = polygons,
  # Keep this list of properties from the polygons.
  properties = list('class'),
  # Set the scale to get Landsat pixels in the polygons.
  scale = 30
)

# Create an SVM classifier with custom parameters.
classifier <- ee$Classifier$libsvm(
  kernelType= 'RBF',
  gamma = 0.5,
  cost = 10
)

# Train the classifier.
trained <- classifier$train(training, 'class', bands)

# Classify the image.
classified <-  image$classify(trained)

# Display the classification result and the input image.
Map$setCenter(-62.836, -9.2399, 9)
Map$addLayer(image, list(bands = c('B4', 'B3', 'B2'), max = 0.5, gamma = 2))
Map$addLayer(polygons, {}, 'training polygons')
Map$addLayer(classified,
             list(min = 0, max = 1, palette = c('red', 'green')),
             'deforestation');
```


# Unsupervised Classification Algorithms 

To start your journey in mastering R, the following six chapters will help you learn the foundational components of R. I expect that you've already seen many of these pieces before, but you probably have not studied them deeply. To help check your existing knowledge, each chapter starts with a quiz; if you get all the questions right, feel free to skip to the next chapter!

# TensorFlow models 

To start your journey in mastering R, the following six chapters will help you learn the foundational components of R. I expect that you've already seen many of these pieces before, but you probably have not studied them deeply. To help check your existing knowledge, each chapter starts with a quiz; if you get all the questions right, feel free to skip to the next chapter!

# TensorFlow examples workflows

To start your journey in mastering R, the following six chapters will help you learn the foundational components of R. I expect that you've already seen many of these pieces before, but you probably have not studied them deeply. To help check your existing knowledge, each chapter starts with a quiz; if you get all the questions right, feel free to skip to the next chapter!

# TFRecord data format

To start your journey in mastering R, the following six chapters will help you learn the foundational components of R. I expect that you've already seen many of these pieces before, but you probably have not studied them deeply. To help check your existing knowledge, each chapter starts with a quiz; if you get all the questions right, feel free to skip to the next chapter!
